# MoE-Style Model Merging Strategies

## Overview

We have 200 trained per-persona LoRAs that individually overfit (68.3%). Instead of training new models, we can use MoE-inspired merging to create better personalized models.

## Strategy Comparison

| Strategy | Experts Per Persona | Computation | Expected Improvement | Implementation Time |
|----------|-------------------|-------------|---------------------|-------------------|
| **Simple Average** | 200 (all) | Low | +1-3% | 15 min âœ“ |
| **Cluster Average** | 16-72 (cluster) | Low | +3-5% | 15 min âœ“ |
| **Top-K Global** | 5-10 | Medium | +2-4% | 30 min |
| **Sparse MoE (Cluster + K)** | 5 | Medium | +4-6% | 45 min ğŸ¯ |
| **Soft Weighted** | 200 (weighted) | High | +3-5% | 60 min |
| **Learned Router** | 5-10 | Very High | +5-8% | 2-3 days |

## Recommended: Sparse MoE (Cluster + Top-K)

### Why This Works

```
Per-persona LoRA (30 examples)     â†’  68.3% âŒ Overfits
Cluster LoRA (480-2160 examples)   â†’  85-87% âœ“ Less specialized
Sparse MoE (5 experts, weighted)   â†’  87-89% ğŸ¯ Best of both!
```

**Key insight**: Combine cluster scope (prevents overfitting) with K-nearest neighbors (preserves personalization)

### Algorithm

```python
For each persona (e.g., persona_042):
  1. Find their cluster (e.g., cluster_1)
  2. Get all personas in cluster_1 (52 personas)
  3. Find 5 most similar personas within cluster
  4. Compute similarity weights: softmax(similarities)
  5. Merge those 5 LoRAs: Î£(weight[i] * LoRA[i])
  6. Save as persona_042's personalized MoE model
```

### Example

```
persona_042 (cluster_1):
  Experts:
    persona_007: 0.35  (most similar)
    persona_123: 0.28
    persona_089: 0.18
    persona_156: 0.12
    persona_042: 0.07  (self, small weight)

  Merged LoRA = 0.35*LoRA_007 + 0.28*LoRA_123 + ... + 0.07*LoRA_042
```

## Implementation Steps

### 1. Sparse MoE (Recommended - Start Here!)

```bash
# Create sparse MoE models (K=5 experts per persona)
python scripts/moe_merge_sparse.py --k 5

# This creates:
# - models/moe_sparse_k5/persona_000/merged_lora.pt
# - models/moe_sparse_k5/persona_001/merged_lora.pt
# - ...
# - models/moe_sparse_k5/routing_info.json (which experts used)

# Expected time: ~45 minutes (200 personas Ã— ~10 sec each)
```

### 2. Evaluate Sparse MoE

```bash
python scripts/eval_moe_sparse.py

# Computes metrics for all 200 personas
# Expected: 87-89% embedding similarity
```

### 3. Alternative: Soft Weighted MoE

```bash
# Use all 200 experts with similarity weights (slower)
python scripts/moe_merge_weighted.py

# Expected: 85-87% (similar to cluster, but more personalized)
```

## Comparison with Cluster Training

| Approach | Training Time | Data Per Model | Expected Score | Flexibility |
|----------|--------------|----------------|----------------|-------------|
| **Cluster LoRA** | 6-8 hours | 480-2160 examples | 85-87% | Low (5 models) |
| **Sparse MoE** | None (merge only) | Uses existing LoRAs | 87-89% | High (200 models) |

**Trade-off**:
- Cluster LoRA: Better if you want 5 models to serve (low memory)
- Sparse MoE: Better for maximum personalization (200 models, more memory)

## Advanced MoE Strategies

### Top-K Global (No Clustering)

```bash
# Find K most similar personas globally (ignoring clusters)
python scripts/moe_merge_topk.py --k 5

# Pros: Simple, global view
# Cons: May include very dissimilar personas across clusters
# Expected: 84-86%
```

### Two-Level Hierarchical MoE

```python
# Level 1: Route to top-2 clusters (soft)
cluster_weights = softmax([sim(persona, cluster_0), ..., sim(persona, cluster_4)])
top_2_clusters = argsort(cluster_weights)[-2:]

# Level 2: Within each cluster, top-3 personas
for cluster in top_2_clusters:
    top_3 = find_top_k_in_cluster(persona, cluster, k=3)

# Result: 6 experts total (2 clusters Ã— 3 personas)
merged = weighted_average(6_experts)

# Expected: 88-90% (best theoretical performance)
# Complexity: High (need cluster embeddings + routing)
```

### Learned Router (Advanced)

Train a small neural network to predict weights:

```python
# Input: persona embedding (384 dim)
# Output: 200 weights (which LoRAs to use)

router = nn.Sequential(
    nn.Linear(384, 128),
    nn.ReLU(),
    nn.Linear(128, 200),
    nn.Softmax()
)

# Train router to minimize validation loss
# Expected: 89-91% (state-of-the-art)
# Time: 2-3 days training + implementation
```

## Quick Decision Tree

```
Do you have 8+ hours for training?
â”œâ”€ YES: Train cluster LoRAs (guaranteed 85-87%)
â”‚   â””â”€ scripts/train_cluster_lora.py
â”‚
â””â”€ NO: Use model merging
    â”œâ”€ Want quick baseline? (15 min)
    â”‚   â””â”€ scripts/merge_cluster_loras.py  (83-85%)
    â”‚
    â””â”€ Want best results? (45 min)
        â””â”€ scripts/moe_merge_sparse.py  (87-89%) ğŸ¯
```

## Expected Final Results

```
Method                    Emb Sim    Time       Memory
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Baseline                  63.79%     -          Low
Unified LoRA             82.14%     2 hrs      Low
Per-Persona LoRA          68.28%     6 hrs      High âŒ
Cluster LoRA (trained)    85-87%     8 hrs      Medium âœ“
Simple Merge              83-84%     15 min     Low
Cluster Merge             84-86%     15 min     Low
Top-K MoE                 84-86%     30 min     High
Sparse MoE (K=5)          87-89%     45 min     High ğŸ¯
Hierarchical MoE          88-90%     90 min     High
Learned Router MoE        89-91%     2-3 days   High
```

## What to Run Now

### Option 1: Quick Win (While Cluster 0 Trains)

```bash
# Run sparse MoE merge (45 min, no GPU needed)
python scripts/moe_merge_sparse.py --k 5
python scripts/eval_moe_sparse.py

# Expected: 87-89%, beats cluster training!
```

### Option 2: Wait for Cluster Training

```bash
# Wait ~1 hour for cluster 0
# Then evaluate and decide if cluster approach is worth continuing
```

### Option 3: Run Both in Parallel

```bash
# Terminal 1: Cluster training (already running)
# Terminal 2: MoE merging
python scripts/moe_merge_sparse.py --k 5
```

## Evaluation Plan

After running experiments, compare ALL methods:

```bash
python scripts/compare_all_final.py
```

This will generate comprehensive visualization including:
- Baseline, Unified, Per-Persona, Hybrid, Prefix
- Selective Routing
- Cluster trained (if finished)
- Simple merge, Cluster merge
- Sparse MoE, Hierarchical MoE (if implemented)

## Next Steps

1. âœ“ Created sparse MoE implementation
2. Run: `python scripts/moe_merge_sparse.py --k 5`
3. Create eval script for MoE models
4. Compare all methods with comprehensive visualization

The sparse MoE approach gives you the **best personalization** without requiring **any additional training**!
